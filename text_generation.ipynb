{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74b0810f-073e-4c8f-9681-6f1d50d36f36",
   "metadata": {},
   "source": [
    "# Text Generation with Python and Keras\n",
    "    Goal :\n",
    "    - Process Text\n",
    "    - Clean Text\n",
    "    - Tokenize the Text and create Sequences with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a2fbfb5-f30f-49bc-8f12-5c474d2b777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    with open(filepath) as f:\n",
    "      str_text = f.read()\n",
    "        \n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1307d69-1f12-4764-9251-883896b8b529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_file('moby_dick_four_chapters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "414bbecd-72e4-4050-9485-914436b0066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "579b6719-7767-47d8-8f6a-7ad891c578ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm',disable=['parser','tagger','ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a333ba4-3d3d-4ba0-8d2b-bd7f210d27ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 1198623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a45e0082-123a-4151-904b-0229a9e4fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punc(doc_text):\n",
    "    exclude = {'#', '$', '\\n', '.', ',', '!', '?', ':', ';', '(', ')', '[', ']', '{', '}', '\"', \"'\", '-', '_', '/', '\\\\', '|', '@', '%', '^', '&', '*', '~', '`', '+'}\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in exclude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fabcd2a-0756-4565-8b44-3cd817bfa19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = read_file('moby_dick_four_chapters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73c6a787-2bc1-47da-8995-cf2fb340696a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Engel\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "tokens = separate_punc(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a77443fa-3ff5-46a5-8eb5-59fbe1ccf3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11851"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ead1e5c1-0787-4ff9-8f54-64b10e739eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 25 + 1\n",
    "text_sequences = []\n",
    "for i in range(train_len,len(tokens)):\n",
    "    seq= tokens[i-train_len:i]\n",
    "    text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "677a2eee-3fd5-4601-9786-7028ddf4be5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6a38d0c-9bf1-49b4-86ed-ad6c7c187631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'call me ishmael   some years ago -- never mind how long precisely -- having little or no money in my purse and nothing particular to'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_sequences[1]\n",
    "' '.join(text_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c562418-fef8-4700-bede-baf3d783a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fd676b7-d029-4aee-8264-6b6e61ab5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "535c0f78-5b74-4896-b1e5-3055295e9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f386b09-b8ba-4460-b79c-4728433d5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2d751fb-4d0b-464e-85af-648cd9353183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b58c25da-1812-4d5a-917c-b70be510f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    " # tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79026862-3e7b-45d9-b6a7-1d84caba9530",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "225ce534-e428-4554-b1a9-a80ce6580309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2721"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7469cf08-7eb6-454c-b269-635df2b2d988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7262681-760c-4d73-bcbe-eb12d723284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb9e8e70-d3e1-422f-b2cd-371f1cbae233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 959,   16,  265, ...,  152,  261,    7],\n",
       "       [  16,  265,    4, ...,  261,    7,  960],\n",
       "       [ 265,    4,   54, ...,    7,  960,   16],\n",
       "       ...,\n",
       "       [ 955,   13,  168, ...,  264,   56,    2],\n",
       "       [  13,  168, 2716, ...,   56,    2, 2721],\n",
       "       [ 168, 2716,    3, ...,    2, 2721,   29]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2252954a-3844-4325-a821-531b7249e529",
   "metadata": {},
   "source": [
    "### From this point, we are going to work on the next steps :\n",
    "    - Create the LSTM-Based Model\n",
    "    - Split the Data into Features and Labels\n",
    "        - X Features (First n words of Sequence)\n",
    "        - Y label (next word after the sequence)\n",
    "    - Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b77d60e-e54c-4f93-a6ce-664b963619ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68468afd-57fe-4c46-aa17-5939ae6682e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequences[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c838ea18-cc41-4094-bece-c78f8c27914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c46934de-337c-4538-96e7-baf5d0aaa11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y =  to_categorical(y,num_classes=vocabulary_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71221de1-543a-44ba-8839-2f7aa081e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c869cbce-0df8-4e72-9e31-6f76e5efe816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11825, 25)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6122d488-35a9-4e58-a2d5-14b562479147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,LSTM,Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93a2ae64-95f8-4f88-8440-55a8ca63ba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_size, seq_len):\n",
    "    model = Sequential()\n",
    "   \n",
    "    model.add(Embedding(input_dim=vocabulary_size, output_dim=100))  # Removed input_length\n",
    "    model.add(LSTM(50, return_sequences=True))\n",
    "    model.add(LSTM(50, activation='relu'))\n",
    "    model.build(input_shape=(None, seq_len))  # Optional\n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75fe5033-eb6f-413d-b626-0c89177f8a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">272,200</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">30,200</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2722</span>)                │         <span style=\"color: #00af00; text-decoration-color: #00af00\">138,822</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m100\u001b[0m)             │         \u001b[38;5;34m272,200\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m50\u001b[0m)              │          \u001b[38;5;34m30,200\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │          \u001b[38;5;34m20,200\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2722\u001b[0m)                │         \u001b[38;5;34m138,822\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">461,422</span> (1.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m461,422\u001b[0m (1.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">461,422</span> (1.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m461,422\u001b[0m (1.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Correctly aligned call\n",
    "model = create_model(vocabulary_size + 1, seq_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad8f693d-7b3a-4809-9679-5d5fc85df6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2bc44f22-fdf9-4152-9677-04a7532bdfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 39ms/step - accuracy: 0.0231 - loss: 7.3814\n",
      "Epoch 2/2\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 44ms/step - accuracy: 0.0495 - loss: 6.2970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21e0b7e95e0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y,batch_size=128,epochs=2,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e9c9f23-d499-41ec-8431-61ebdbd0d409",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "400b31c4-39c5-4f25-a444-792e628165da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(tokenizer,open('my_simpletokenizer','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5cf721e-16d8-448d-9b82-c65292dc4108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04880c1b-1d9e-4f91-8d7b-97b9ae4152c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model,tokenizer,seq_len,seed_text,num_gen_words):\n",
    "    output_text = []\n",
    "    input_text =seed_text\n",
    "    for i in range(num_gen_words):\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "        pred_word_ind = model.predict(pad_encoded, verbose=0).argmax(axis=-1)[0]\n",
    "        pred_word = tokenizer.index_word[pred_word_ind]\n",
    "        input_text +=' '+pred_word\n",
    "        output_text.append(pred_word)\n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "097f6fea-d147-4e87-903a-9125729ba190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['call',\n",
       " 'me',\n",
       " 'ishmael',\n",
       " ' ',\n",
       " 'some',\n",
       " 'years',\n",
       " 'ago',\n",
       " '--',\n",
       " 'never',\n",
       " 'mind',\n",
       " 'how',\n",
       " 'long',\n",
       " 'precisely',\n",
       " '--',\n",
       " 'having',\n",
       " 'little',\n",
       " 'or',\n",
       " 'no',\n",
       " 'money',\n",
       " 'in',\n",
       " 'my',\n",
       " 'purse',\n",
       " 'and',\n",
       " 'nothing',\n",
       " 'particular',\n",
       " 'to']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "391feaa1-60df-4920-b54f-70c9038391de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(101)\n",
    "random_pick = random.randint(0, len(text_sequences) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d22045b-18e7-4059-8a5e-a2619ed96f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed_text = text_sequences[random_pick]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "60c4cc6b-cd3d-40e7-bb3b-c6a73fa0b9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['so',\n",
       " 'long',\n",
       " 'been',\n",
       " 'bound',\n",
       " '\\n\\n',\n",
       " 'but',\n",
       " 'the',\n",
       " 'interval',\n",
       " 'i',\n",
       " 'spent',\n",
       " 'in',\n",
       " 'deliberating',\n",
       " 'what',\n",
       " 'to',\n",
       " 'say',\n",
       " 'was',\n",
       " 'a',\n",
       " 'fatal',\n",
       " 'one',\n",
       " ' ',\n",
       " 'taking',\n",
       " 'up',\n",
       " 'his',\n",
       " 'tomahawk',\n",
       " 'from',\n",
       " 'the']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a29cb8bd-b3cb-4195-a53d-eebdb34f98d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = ' '.join(random_seed_text).replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d24f963f-3ed9-4ebe-abfc-2e5b426dd38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'so long been bound  but the interval i spent in deliberating what to say was a fatal one   taking up his tomahawk from the'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dcdc6f18-4ec7-45ac-a82a-e1294bbeed84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the the the the the the the the the the the the the the the the the the the the the the the the the'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
